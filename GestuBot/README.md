# GestuBot

Real-time hand gesture recognition system that maps 5 gesture classes to keyboard inputs (WASD) and controls a 3D robot arm in the browser. Built with OpenCV + scikit-learn.

The full pipeline — data collection, feature extraction, SVM training, live inference — runs at roughly **15–20 ms per frame**, well within the <100 ms real-time threshold.

## How It Works

Camera feed goes through an HSV skin mask and morphological cleanup to isolate the hand. The largest contour gets passed to a feature extractor that computes a 13-dimensional vector:

- **Geometric ratios** (3): aspect ratio, extent, solidity — captures hand shape independent of size
- **Hull defect count** (1): number of significant convexity defects, correlates with extended fingers
- **Hu Moments** (7): log-transformed, rotation/scale invariant shape descriptors
- **Normalized center of mass** (2): helps distinguish left-pointing vs right-pointing gestures

These features feed into an SVM (RBF kernel) trained with GridSearchCV. A 5-frame rolling buffer does mode filtering to prevent jittery outputs.

```
Camera → HSV Mask → Contour → Feature Extraction (13-dim) → SVM → Rolling Buffer → Keyboard / WebSocket
```

## Project Structure

```
GestuBot/
├── src/
│   ├── data_collector.py   # collect + label training samples with HSV tuning
│   ├── trainer.py          # SVM training, GridSearchCV, evaluation metrics
│   ├── inference.py        # real-time inference loop with latency profiler
│   ├── utils.py            # shared feature extraction + vision pipeline
│   └── ws_server.py        # WebSocket server for browser visualization
├── web/
│   └── robot_arm.html      # Three.js robot arm controlled via WebSocket
├── data/                   # training data (generated by data_collector)
├── models/                 # trained model + confusion matrix (generated by trainer)
└── requirements.txt
```

## Getting Started

### Install

```bash
git clone https://github.com/<your-username>/GestuBot.git
cd GestuBot
pip install -r requirements.txt
```

### Collect Training Data

```bash
cd src
python data_collector.py
```

Use the HSV trackbars to isolate your hand from the background. Press `0`-`5` to label samples for each gesture class. Try to get at least 50 samples per class.

| Key | Gesture | Mapped Key |
|-----|---------|------------|
| 0 | Fist | Release all (stop) |
| 1 | Open Palm | W (forward) |
| 2 | Point Left | A (left) |
| 3 | Point Right | D (right) |
| 4 | V-Sign | S (reverse) |
| 5 | Background | None |

Press `q` when done — dataset saves to `data/gestures.csv`.

### Train

```bash
python trainer.py
```

Runs GridSearchCV over `C` and `gamma` with 5-fold stratified cross-validation. Outputs the trained model to `models/gesture_svm.joblib` and a confusion matrix to `models/confusion_matrix.png`.

### Run Inference

```bash
python inference.py
```

Shows live camera feed with contour overlay, predicted gesture, per-frame latency, and a performance indicator. Press `q` to quit — prints a full latency summary before exiting.

### 3D Robot Arm (optional)

While inference is running, open `web/robot_arm.html` in a browser. It connects over WebSocket and moves the arm based on your gestures:

- Fist → reset position
- Palm → extend arm
- Point Left/Right → rotate
- V-Sign → toggle gripper

## Latency Benchmarking

There's a headless benchmark mode that skips the GUI and just measures pipeline latency:

```bash
python src/inference.py --benchmark --frames 300
```

Prints per-stage breakdown (preprocess, contour detection, classification, debouncing) with mean, median, p95, p99. This is how I validated the <20ms claim.

## Some Design Decisions

**Why SVM over a neural net?** For a 13-dim feature vector with 5 classes, an SVM is more than sufficient and the inference time is basically zero compared to the vision pipeline. No GPU dependency either.

**Why engineered features instead of raw pixels?** Hu Moments + geometric ratios give rotation/scale invariance out of the box. A CNN would need way more data and compute for marginal gains on this problem.

**StandardScaler in the pipeline is critical.** Hu Moments live in the 10⁻³ to 10⁻⁷ range while geometric ratios are 0-2. Without normalization the SVM basically ignores the moments entirely.

**Face shield.** The vision pipeline masks out the top 30% of the frame to prevent false detection when your face is in view (skin tone overlap). Simple but effective.

**Debouncing tradeoff.** The 5-frame buffer adds ~166ms of latency at 30 FPS, but it eliminates the jittering problem where predictions flip between classes frame-to-frame. Worth it for usable keyboard control.

## Requirements

- Python 3.8+
- Webcam
- Dependencies in [requirements.txt](requirements.txt)

## License

MIT
